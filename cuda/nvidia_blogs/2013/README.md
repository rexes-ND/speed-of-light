## [CUDA Pro Tip: Flush Denormals with Confidence](https://developer.nvidia.com/blog/cuda-pro-tip-flush-denormals-confidence/)

This blog is about denormal floating point numbers.
The denormals allows what is known as "gradual underflow" when a result is too small, and helps avoid catastrophic division-by zero errors.
Gradual or graceful underflow is using denormals to let number fade smoothly toward 0 instead of abruptly snapping to zero.
In GPU, there are some cases where it has to take a slower path for denormal values.
One way to avoid denormals is to add small "noise" so that the number is guaranteed to be denormal.
`nvcc` provides the cmdline option `-ftz=true` which causes all denormalized numbers to be flushed to zero.

## [How to Access Global Memory Efficiently in CUDA C/C++ Kernels](https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/)

[Code](src/gmem_access.cu)

The blog explored kernels that are shifted with fixed offsets and that are strided.

The most important take-away from this blog is that the device coalesces global memory loads and stores issued by warp into as few transactions as possible to minimize DRAM bandwidth.

Arrays allocated in device memory (`cudaMalloc`) are aligned to 256B memory segments by the CUDA driver.

On modern GPU architecture, device memory access with offset has no issue but strided memory access can hurt performance.

Single Precision: ![Single Precision](misc/bandwidth_single.png)
Double Precision: ![Double Precision](misc/bandwidth_double.png)

## [Using Shared Memory in CUDA C/C++](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)

[Code](src/smem.cu)

The blog introduces the shared memory:

1. Sharing data over shared memory using `__syncthreads()`.
2. Some hardware details including memory banks and broadcast.
3. CUDA API(s) related to shared memory configuration.

## [An Efficient Matrix Transpose in CUDA C/C++](https://developer.nvidia.com/blog/efficient-matrix-transpose-cuda-cc/)

[Code](src/matrix_transpose.cu)

The blog is about transposing matrix using the shared memory.

The highlight of this blog is using extra padding to avoid shared memory bank conflict.

## [Finite Difference Methods in CUDA C/C++, Part 1](https://developer.nvidia.com/blog/finite-difference-methods-cuda-cc-part-1/)

[Code](src/finite_difference.cu)

The blog is about finite difference method.

The highlight of this blog is using shared memory to reuse function values and using constant memory for coefficients.

Constant memory resides in device DRAM and is cached on chip.
The constant memory cache has only **one read port** but can broadcast data from this port across a warp.
This means that constant memory is effective when all threads in a warp read the same address, but when threads in a warp read different addresses the reads are serialized.

## [Finite Difference Methods in CUDA C++, Part 2](https://developer.nvidia.com/blog/finite-difference-methods-cuda-c-part-2/)

[Code](src/finite_difference.cu)

The blog is part 2 of the previous blog.

This blog also explored calculating occupancy from register usage (`--ptxas-options=-v`) and number of threads per thread block.

## [CUDA Pro Tip: Write Flexible Kernels with Grid-Stride Loops](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/)

This blog recommend the use of a grid-stride loop instead of a monolithic kernel.

```cpp
// monolithic kernel
// WARNING: This assumes that a single large grid of threads processes the entire array in one pass
int i = blockIdx.x * blockDim.x + threadIdx.x;
if (i < n)
    // ...


// grid-stride loop
// Benefits: scalable, thread reuse, and <<<1, 1>>> for serial execution, which is useful for debugging
for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < n; i += blockDim.x * gridDim.x) {
    // ...
}
```

## [CUDA Pro Tip: Understand Fat Binaries and JIT Caching](https://developer.nvidia.com/blog/cuda-pro-tip-understand-fat-binaries-jit-caching/)

`nvcc`, the CUDA compiler driver, uses a two-stage compilation model:

1. Compiling source code to PTX **virtual** assembly, which is forward compatible
2. Compiling PTX to binary code (SASS) for the target architecture, which can be done by the CUDA driver at run time

The JIT compilation can cause delay at application start-up time (or more accurately, CUDA context creation time).

CUDA uses 2 approaches to mitigate start-up overhead on JIT complation: fat binaries and JIT caching.

The first approach is to completely avoid JIT cost by including SASS in the application binary along with PTX code.
`nvcc` organizes device code into "fat binaries", which are able to hold multiple translations of the same GPU source code.

The second approach to mitigate JIT overhead is to cache the binaries generated by JIT compilation.
The cache (a.k.a compute cache) is automatically invalidated when the device driver is upgraded, so that apps can benefit from improvements in the JIT compiler built into the device driver.

- `CUDA_CACHE_DISABLE` can be used to disable caching
- `CUDA_CACHE_MAXSIZE` specifies the size of the compute cache in bytes
- `CUDA_CACHE_PATH` specifies the directory location of compute cache files
- `CUDA_FORCE_PTX_JIT` can be used to always force JIT compilation (helpful for verifying if fatbin contains PTX)

## [Unified Memory in CUDA 6](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/)

Unified memory creates a pool of managed memory that shared between the CPU and GPU, bridging the CPU-GPU divide.
Managed memory is accessible to both CPU and GPU using a single pointer.
The system automatically migrates data allocated in Unified Memory (`cudaMallocManaged`) between host and device so that it looks like CPU memory to code running on the CPU, and like GPU memory to code running on the GPU.

Unified Memory lowers the bar of entry to parallel programming on the CUDA platform, by making device memory management an optimization, rather than a requirement.

Unified Memory depends on UVA (Unified Virtual Addressing).
UVA provides a single virtual memory address space for all memory system.
It allows `cudaMemcpy` to be used without specifying where exactly the input and output parameters reside.
UVA enables "Zero-Copy" memory, which is pinned host memory accessible by device code directly, over PCI-Express, without a memcpy.

Unified Memory is able to automatically migrate data at the level of individual pages between host and device memory.

A key benefit of Unified Memory is simplifying the heterogeneous computing memory model by eliminating the need for deep copies when accessing structured data in GPU kernels.
